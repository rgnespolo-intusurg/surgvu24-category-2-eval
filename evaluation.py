from evalutils import ClassificationEvaluation
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.preprocessing import LabelEncoder
import json
import pandas as pd
import glob
import os
import logging
from typing import List

LOG = logging.getLogger(__name__)
logging.basicConfig(format='[%(filename)s:%(lineno)s - %(funcName)20s()] %(asctime)s %(message)s', level="INFO")


class SurgVU(ClassificationEvaluation):
    def __init__(self):
        self._gt_json_list = self.get_list_gt_jsons()
        self._pred_json_list, self._name_map  = self.get_list_pred_jsons()
        logging.info("Found the following name mapping: " + str(self._name_map))


    def get_list_gt_jsons(self):
        gt_jsons = glob.glob('true_jsons/*.json')
        logging.info(" Found the following ground truth jsons:")
        logging.info("    " + str(gt_jsons))
        return gt_jsons
    


    def get_list_pred_jsons(self, pred_json_loc="/input/predictions.json"):
        if os.path.isfile(pred_json_loc):
            logging.info("    found the prediction json location: " + pred_json_loc)
            cases = self.load_predictions_json(fname=pred_json_loc)
            return list(cases.keys()), cases
        else:
            logging.info("     did not find prediciton json location...")
            logging.info("     loading examples predictions from local repo...")
            pred_jsons = glob.glob('inference_output/*.json')
            return pred_jsons, None


    
    def load_predictions_json(self, fname="/input/predictions.json"):

        cases = {}

        with open(fname, "r") as f:
            entries = json.load(f)
        f.close()

        if isinstance(entries, float):
            raise TypeError(f"entries of type float for file: {fname}")

        # logging.info("  Here's the structure of the predictions.json file generated by GC...")
        # logging.info(str(entries))
        
        for e in entries:
            # Find case name through input file name
            inputs = e["inputs"]
            name = None
            for input in inputs:
                if input["interface"]["slug"] == "endoscopic-robotic-surgery-video":
                    name = str(input["file"]).split('/')[-1]
                    logging.info("   found input file: " + name)
                    break  # expecting only a single input
            if name is None:
                raise ValueError(f"No filename found for entry: {e}")

            entry = {"name": name}

            # Find output value for this case
            outputs = e["outputs"]

            for output in outputs:
                pk = e["pk"]
                relative_path = output["interface"]["relative_path"]
                #full_path_output_json = "mock_input/" + relative_path

                full_path_output_json = "/input/" + pk + "/output/" + relative_path
                cases[full_path_output_json] = name

        return cases


    def evaluate_all_gt(self, gt_json_list, pred_json_list, name_map=None):
        results = []
        for gt_file in gt_json_list:
            gt_name = gt_file.split('/')[-1].split('.json')[0]
            found_match = False
            for pred_file in pred_json_list:
                if name_map:
                    true_pred_name = name_map[pred_file].split('.')[0]
                else:
                    true_pred_name = pred_file.split('.')[-1][0]
                if gt_name == true_pred_name:
                    found_match = True
                    break
            if found_match:
                logging.info("Matched " + gt_name + " with " + true_pred_name + "...")
                pred_data = self.load_json(pred_file)
                gt_data = self.load_json(gt_file)
                result = self.evaluate_single_video(gt_data, pred_data)
                results.append(result)
            else:
                logging.info("Could not match " + gt_name + " with any prediction files...")
                logging.info("Not evaluating on this file...")
        return pd.DataFrame(results)

    



    
    def load_json(self, json_file: str) -> dict:
        with open(json_file, "r") as f:
            data = json.load(f)
        return pd.DataFrame(data)



    def evaluate_single_video(self, gt_data: pd.DataFrame, pred_data: pd.DataFrame) -> dict:


        combined = pd.merge(gt_data, pred_data, on="frame_nr", how="left")
        combined['true'] = combined['surgical_step_x']
        combined['pred'] = combined['surgical_step_y']
        combined['pred'] = combined['pred'].fillna(7)
        combined['true'] = combined['true'].fillna(7)
        
        results = {
            "accuracy": accuracy_score(combined['true'], combined['pred']),
            "f1": f1_score(combined['true'], combined['pred'], average="weighted"),
            "precision": precision_score(combined['true'], combined['pred'], average="weighted"),
            "recall": recall_score(combined['true'], combined['pred'], average="weighted")
        }
        return results

    def evaluate(self):
        results = self.evaluate_all_gt(self._gt_json_list, self._pred_json_list, name_map=self._name_map)
        summary_dict = {
            "accuracy": float(results['accuracy'].mean()),
            "f1": float(results['f1'].mean()),
            "precision": float(results['precision'].mean()),
            "recall": float(results['recall'].mean())
        }
        if os.path.isdir("/output/"):
            filename = "/output/metrics.json"
            LOG.info("Writing evaluation results to /output/metrics.json")
            with open(filename, 'w') as f:
                json.dump(summary_dict, f, indent=4)
        else:
            LOG.info("No /output/ directory found, evaluation results will not be written to a file.")
            LOG.info(summary_dict)



if __name__ == "__main__":
    evaluator = SurgVU()
    evaluator.evaluate()
